"""
PyTorch Lightning DataModule for character segmentation.
Handles data loading, splitting, and batching for training.
"""

from typing import Optional, Dict, Any
from pathlib import Path

import pytorch_lightning as pl
from torch.utils.data import DataLoader

from data.dataset import (
    CharacterSegmentationDataset,
    CharacterSegmentationInferenceDataset,
    collate_fn,
    get_dataset
)


class CharacterSegmentationDataModule(pl.LightningDataModule):
    """
    Lightning DataModule for character segmentation.
    Manages train/val/test datasets and dataloaders.
    """
    
    def __init__(
        self,
        data_dir: str,
        config: Dict[str, Any],
        batch_size: Optional[int] = None,
        num_workers: Optional[int] = None,
        pin_memory: Optional[bool] = None
    ):
        """
        Initialize DataModule.
        
        Args:
            data_dir: Root directory containing the dataset
            config: Configuration dictionary
            batch_size: Batch size (overrides config if provided)
            num_workers: Number of data loading workers (overrides config)
            pin_memory: Whether to pin memory (overrides config)
        """
        super().__init__()
        
        self.data_dir = Path(data_dir)
        self.config = config
        
        training_config = config.get('training', {})
        self.batch_size = batch_size or training_config.get('batch_size', 8)
        self.num_workers = num_workers or training_config.get('num_workers', 4)
        self.pin_memory = pin_memory if pin_memory is not None else training_config.get('pin_memory', True)
        
        self.val_batch_size = config.get('validation', {}).get('val_batch_size', self.batch_size)
        
        self.train_dataset: Optional[CharacterSegmentationDataset] = None
        self.val_dataset: Optional[CharacterSegmentationDataset] = None
        self.test_dataset: Optional[CharacterSegmentationDataset] = None
    
    def prepare_data(self) -> None:
        """
        Download or prepare data (called only on one GPU/process).
        For this project, data is already generated by generate_dataset.py
        """
        if not self.data_dir.exists():
            raise FileNotFoundError(
                f"Data directory not found: {self.data_dir}\n"
                f"Please run: python generate_dataset.py --num_samples 1000"
            )
        
        required_files = [
            self.data_dir / "dataset_splits.json",
            self.data_dir / "dataset_summary.json"
        ]
        
        for file_path in required_files:
            if not file_path.exists():
                raise FileNotFoundError(
                    f"Required file not found: {file_path}\n"
                    f"Please run generate_dataset.py to create the dataset."
                )
    
    def setup(self, stage: Optional[str] = None) -> None:
        """
        Setup datasets for each stage (fit, validate, test, predict).
        
        Args:
            stage: Current stage ('fit', 'validate', 'test', 'predict')
        """
        if stage == 'fit' or stage is None:
            self.train_dataset = get_dataset(
                data_dir=str(self.data_dir),
                split='train',
                config=self.config,
                training=True
            )
            
            self.val_dataset = get_dataset(
                data_dir=str(self.data_dir),
                split='val',
                config=self.config,
                training=False
            )
            
            self._print_dataset_info()
        
        if stage == 'validate' or stage is None:
            if self.val_dataset is None:
                self.val_dataset = get_dataset(
                    data_dir=str(self.data_dir),
                    split='val',
                    config=self.config,
                    training=False
                )
        
        if stage == 'test' or stage is None:
            self.test_dataset = get_dataset(
                data_dir=str(self.data_dir),
                split='test',
                config=self.config,
                training=False
            )
    
    def train_dataloader(self) -> DataLoader:
        """
        Create training dataloader.
        
        Returns:
            DataLoader for training
        """
        if self.train_dataset is None:
            raise RuntimeError("Train dataset not initialized. Call setup() first.")
        
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            collate_fn=collate_fn,
            persistent_workers=True if self.num_workers > 0 else False,
            drop_last=True
        )
    
    def val_dataloader(self) -> DataLoader:
        """
        Create validation dataloader.
        
        Returns:
            DataLoader for validation
        """
        if self.val_dataset is None:
            raise RuntimeError("Val dataset not initialized. Call setup() first.")
        
        return DataLoader(
            self.val_dataset,
            batch_size=self.val_batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            collate_fn=collate_fn,
            persistent_workers=True if self.num_workers > 0 else False
        )
    
    def test_dataloader(self) -> DataLoader:
        """
        Create test dataloader.
        
        Returns:
            DataLoader for testing
        """
        if self.test_dataset is None:
            raise RuntimeError("Test dataset not initialized. Call setup() first.")
        
        return DataLoader(
            self.test_dataset,
            batch_size=self.val_batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            collate_fn=collate_fn,
            persistent_workers=True if self.num_workers > 0 else False
        )
    
    def _print_dataset_info(self) -> None:
        """Print dataset statistics."""
        print("\n" + "=" * 60)
        print("DATASET INFORMATION")
        print("=" * 60)
        
        if self.train_dataset is not None:
            train_stats = self.train_dataset.get_dataset_statistics()
            print(f"\nTraining Set:")
            print(f"  Samples: {train_stats['num_samples']}")
            print(f"  Total characters: {train_stats['total_characters']}")
            print(f"  Avg chars/image: {train_stats['avg_chars_per_image']:.1f}")
            print(f"  Char range: {train_stats['min_chars']}-{train_stats['max_chars']}")
        
        if self.val_dataset is not None:
            val_stats = self.val_dataset.get_dataset_statistics()
            print(f"\nValidation Set:")
            print(f"  Samples: {val_stats['num_samples']}")
            print(f"  Total characters: {val_stats['total_characters']}")
            print(f"  Avg chars/image: {val_stats['avg_chars_per_image']:.1f}")
        
        print(f"\nDataLoader Configuration:")
        print(f"  Batch size (train): {self.batch_size}")
        print(f"  Batch size (val): {self.val_batch_size}")
        print(f"  Num workers: {self.num_workers}")
        print(f"  Pin memory: {self.pin_memory}")
        print("=" * 60 + "\n")
    
    def get_num_classes(self) -> int:
        """
        Get number of segmentation classes.
        
        Returns:
            Number of classes (max instance ID + 1)
        """
        if self.train_dataset is None:
            return self.config.get('model', {}).get('unet', {}).get('num_classes', 64)
        
        max_instances = 0
        for sample_id in self.train_dataset.samples[:100]:
            mask = self.train_dataset._load_mask(sample_id)
            max_instances = max(max_instances, mask.max())
        
        return int(max_instances) + 1
    
    def get_sample(self, split: str = 'train', idx: int = 0) -> Dict[str, Any]:
        """
        Get a single sample for visualization or debugging.
        
        Args:
            split: Dataset split ('train', 'val', 'test')
            idx: Sample index
            
        Returns:
            Sample dictionary
        """
        if split == 'train' and self.train_dataset is not None:
            return self.train_dataset[idx]
        elif split == 'val' and self.val_dataset is not None:
            return self.val_dataset[idx]
        elif split == 'test' and self.test_dataset is not None:
            return self.test_dataset[idx]
        else:
            raise ValueError(f"Dataset for split '{split}' not initialized")


class InferenceDataModule(pl.LightningDataModule):
    """
    DataModule for inference on real handwritten documents.
    """
    
    def __init__(
        self,
        image_paths: list,
        config: Dict[str, Any],
        batch_size: int = 4,
        num_workers: int = 2
    ):
        """
        Initialize inference DataModule.
        
        Args:
            image_paths: List of paths to images
            config: Configuration dictionary
            batch_size: Batch size for inference
            num_workers: Number of data loading workers
        """
        super().__init__()
        
        self.image_paths = image_paths
        self.config = config
        self.batch_size = batch_size
        self.num_workers = num_workers
        
        self.predict_dataset: Optional[CharacterSegmentationInferenceDataset] = None
    
    def setup(self, stage: Optional[str] = None) -> None:
        """Setup inference dataset."""
        from data.augmentations import get_augmentation_pipeline
        
        if stage == 'predict' or stage is None:
            transform = get_augmentation_pipeline(self.config, training=False)
            
            image_size = tuple(
                self.config.get('dataset_generation', {}).get('image_size', [1024, 1024])
            )
            
            self.predict_dataset = CharacterSegmentationInferenceDataset(
                image_paths=self.image_paths,
                transform=transform,
                image_size=image_size
            )
    
    def predict_dataloader(self) -> DataLoader:
        """
        Create dataloader for inference.
        
        Returns:
            DataLoader for prediction
        """
        if self.predict_dataset is None:
            raise RuntimeError("Predict dataset not initialized. Call setup() first.")
        
        return DataLoader(
            self.predict_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=True
        )


def create_datamodule(config: Dict[str, Any]) -> CharacterSegmentationDataModule:
    """
    Factory function to create DataModule from configuration.
    
    Args:
        config: Configuration dictionary
        
    Returns:
        CharacterSegmentationDataModule instance
    """
    data_dir = config.get('paths', {}).get('synthetic_data_dir', 'outputs/synthetic_data')
    
    datamodule = CharacterSegmentationDataModule(
        data_dir=data_dir,
        config=config
    )
    
    return datamodule